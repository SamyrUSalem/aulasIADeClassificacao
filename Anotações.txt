No momento de validar modelos de classificação, precisamos checar se o modelo está de fato generalizando, ou seja, se está compreendendo o padrão dos dados e classificando corretamente dados novos. A estratégia mais simples para avaliar essa generalização, conhecida como holdout, consiste em dividir os dados em duas partes: um conjunto de dados de treinamento e outro de teste. O conjunto de treinamento é utilizado para treinar o modelo, enquanto o conjunto de teste é usado para avaliar o desempenho do modelo em dados não vistos anteriormente.

Em alguns casos, especialmente quando se realiza ajustes finos nos parâmetros do modelo, é útil ter um conjunto de validação adicional. Nesse caso, a divisão é feita em três partes: conjunto de treino, conjunto de validação e conjunto de teste. O conjunto de validação é usado na comparação de diferentes modelos, na seleção do modelo mais adequado e no ajuste dos hiperparâmetros. Enquanto isso, o conjunto de teste continua sendo utilizado para avaliar o desempenho final do modelo escolhido, após todo o processo de ajuste.

Por isso, quanto mais se utiliza os mesmos dados para tomar decisões sobre configurações de melhorias no modelo ou escolha de hiperparâmetros, mais comprometida se torna a confiabilidade desses resultados ao serem generalizados para dados novos e não vistos. Isso ocorre pois as melhorias são feitas a partir desses dados de validação.

É possível perceber que as melhorias aplicadas desempenham um papel fundamental para resolver o problema. Entretanto, para assegurar que o desempenho do modelo permaneça consistente em relação aos dados do mundo real, que não foram vistos no treinamento ou no aprimoramento dos modelos, a estratégia da divisão entre 3 conjuntos de dados, como se pode analisar na imagem a seguir, oferece um bom direcionamento final, já que indica se o modelo escolhido está viesado ou não em relação aos dados de validação.


Matriz de confusão
Para obter uma avaliação mais completa do desempenho de modelos de classificação, podemos utilizar uma ferramenta conhecida como matriz de confusão. Essa matriz oferece vantagens à pessoa cientista de dados, pelo fato de permitir entender qual a quantidade de erros e acertos das previsões de um modelo. Ao invés de uma taxa de acerto geral, a matriz é capaz de fornecer informações em uma visualização para cada uma das categorias da variável alvo.

Esses valores são muito úteis para uma análise mais aprofundada do modelo de classificação. Isso permite identificar as capacidades e limitações da predição, se há um equilíbrio entre os acertos e erros ou se o resultado está tendencioso para uma classe em detrimento da outra. Com isso, é nítido que a matriz de confusão é uma ferramenta muito mais completa do que a métrica de acurácia, que representa apenas a porcentagem de acerto do modelo, sem considerar as classes de maneira isolada.


As principais métricas de classificação são:

Acurácia
É a métrica mais comum e básica em problemas de classificação. É utilizada para medir a proporção de dados previstos corretamente pelo modelo em relação ao total dos dados. Essa métrica é útil quando as classes da variável alvo estão balanceadas, ou seja, quando existe uma quantidade equilibrada de dados para cada classe e uma importância equivalente de classificação entre as categorias. Como exemplo de utilização, temos o reconhecimento de dígitos manuscritos. Podemos utilizar imagens de caracteres de letras e números para treinar um modelo de classificação para identificar corretamente a escrita. Como cada letra ou número não tem uma importância maior do que as demais, a acurácia se torna uma boa métrica para medir a capacidade do modelo em classificar corretamente os dígitos.

Revocação (recall)
Mede a proporção de dados positivos que foram corretamente identificados pelo modelo, ou seja, revela a capacidade do modelo em evitar a classificação incorreta de dados positivos como negativos. É usada quando o risco ou custo de classificar falsos negativos é alto. Por exemplo, em casos de diagnóstico de doenças graves, em que é fundamental detectar corretamente a presença da doença.

Precisão
Mede a proporção de dados classificados como positivos que são realmente positivos, ou seja, revela a capacidade do modelo em evitar a classificação incorreta de dados negativos como positivos. É usada quando o risco ou custo de classificar falsos positivos é alto, por exemplo em casos de seleção de ações no mercado financeiro, onde o importante é selecionar ações que tenham grande probabilidade de retorno, abaixando a quantidade de ações ruins (falsos positivos) mesmo que outras boas ações não tenham sido detectadas pelo modelo (falso negativo). A precisão também é importante no exemplo de detecção de doenças, onde queremos evitar que pacientes saudáveis sejam erroneamente classificados como doentes.

F1-Score
Fornece um equilíbrio entre o recall e a precisão, sendo útil quando as classes da variável alvo estão desbalanceadas, ou seja, quando há uma quantidade de dados muito diferente para cada classe. Além disso, é aplicável quando o risco ou custo de falsos positivos e de falsos negativos é alto simultaneamente. Em casos de detecção de tumores em pacientes, é preciso ter um equilíbrio entre evitar erros na detecção de tumores quando a pessoa realmente os possui e evitar erros ao informar que uma pessoa possui um tumor quando na realidade ela não possui.


Curva x recall
Como a curva de precisão x recall não avalia a taxa de verdadeiro negativos, que geralmente vai conter a maior quantidade de dados, a análise é mais concentrada na classe com menor quantidade de dados apenas, e isso faz com que a análise se torne melhor em dados desbalanceados.

Relatorio de Avaliação(Modelo)
Quando se trata de avaliar um modelo de classificação, não existe apenas uma opção para entender o comportamento do modelo e entender se está se saindo bem ou não para o problema proposto. Existem muitas métricas distintas, cada uma com sua característica e que podem ser utilizadas em conjunto para a decisão final da análise.

Uma maneira muito simples de resumir o resultado de um modelo é a partir do classification_report ou relatório de classificação. Nele, é possível encontrar as principais métricas em uma única tabela.

O relatório apresenta a precisão, recall e f1-score de cada classe, além de retornar uma média dessas métricas de forma geral ou ponderada em relação a quantidade de dados de cada classe.


Kfold
O processo de validação cruzada consiste na criação de diversos modelos, um para cada divisão feita, utilizando 1 parte para validação e as demais para treino. Em cada um dos modelos é utilizado uma parte diferente de validação. Ao final é possível extrair uma média e intervalo de confiança com os resultados dos modelos. 

GroupKFold
O método GroupKFold é uma variação da validação cruzada KFold tradicional e é utilizado quando os dados possuem alguma estrutura de grupo ou dependência que não deve ser quebrada, geralmente uma característica em uma das colunas da base de dados.

Essa abordagem utiliza uma estratégia de separação dos dados para que os registros pertencentes a um grupo específico sejam mantidas juntas durante as divisões do KFold, garantindo que não sejam separadas entre os conjuntos de treinamento e validação. Isso é útil para evitar possíveis vieses e garantir que o modelo generalize para grupos desconhecidos, ou seja. Mesmo que não tenha um dado do grupo no conjunto de treinamento, o modelo precisará ter um bom desempenho ao prever o resultado para os dados daquele grupo.

Leave-p-out
O método Leave-p-out funciona de maneira diferente do método KFold. Ao invés de dividir o conjunto de dados em uma quantidade fixada de conjuntos, será escolhida uma quantidade 'p' de elementos para serem deixados de fora do treinamento. Os dados serão treinados em todo o restante e validados apenas nos 'p' elementos. Esse processo é repetido até que todos os dados sejam utilizados como dados de validação. O resultado final pode ser considerado a média dos resultados obtidos nos modelos, assim como é feito na validação cruzada tradicional.

Isso proporciona uma validação bem mais completa, já que considera todas as combinações possíveis de dados de treino e validação. Porém, é muito mais custosa computacionalmente, já que serão criados muitos modelos e isso aumenta a quantidade conforme o conjunto de dados seja muito grande e o valor escolhido para 'p' seja pequeno.

Leave-one-out
O método Leave-one-out é uma forma especial do Leave-p-out, onde é escolhido o valor de p=1. Dessa maneira, apenas uma amostra é reservada para validação e todos os outros dados são escolhidos para treinamento. Esse processo é repetido para todas as amostras da base de dados. Isso significa que, se houver 1000 linhas na base de dados, serão treinados 1000 modelos distintos.

É de se esperar que esse método demanda muito computacionalmente, pela criação de um modelo para cada linha da base de dados. Portanto, ele é indicado somente nos casos em que a base de dados é muito pequena. Nessas situações, é interessante utilizar a maior quantidade de dados possível para treinamento, para que o modelo consiga entender o padrão dos dados. Outra estratégia de validação cruzada removeria muitos dados que seriam úteis no treino.