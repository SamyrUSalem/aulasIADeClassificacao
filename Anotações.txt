No momento de validar modelos de classificação, precisamos checar se o modelo está de fato generalizando, ou seja, se está compreendendo o padrão dos dados e classificando corretamente dados novos. A estratégia mais simples para avaliar essa generalização, conhecida como holdout, consiste em dividir os dados em duas partes: um conjunto de dados de treinamento e outro de teste. O conjunto de treinamento é utilizado para treinar o modelo, enquanto o conjunto de teste é usado para avaliar o desempenho do modelo em dados não vistos anteriormente.

Em alguns casos, especialmente quando se realiza ajustes finos nos parâmetros do modelo, é útil ter um conjunto de validação adicional. Nesse caso, a divisão é feita em três partes: conjunto de treino, conjunto de validação e conjunto de teste. O conjunto de validação é usado na comparação de diferentes modelos, na seleção do modelo mais adequado e no ajuste dos hiperparâmetros. Enquanto isso, o conjunto de teste continua sendo utilizado para avaliar o desempenho final do modelo escolhido, após todo o processo de ajuste.

Por isso, quanto mais se utiliza os mesmos dados para tomar decisões sobre configurações de melhorias no modelo ou escolha de hiperparâmetros, mais comprometida se torna a confiabilidade desses resultados ao serem generalizados para dados novos e não vistos. Isso ocorre pois as melhorias são feitas a partir desses dados de validação.

É possível perceber que as melhorias aplicadas desempenham um papel fundamental para resolver o problema. Entretanto, para assegurar que o desempenho do modelo permaneça consistente em relação aos dados do mundo real, que não foram vistos no treinamento ou no aprimoramento dos modelos, a estratégia da divisão entre 3 conjuntos de dados, como se pode analisar na imagem a seguir, oferece um bom direcionamento final, já que indica se o modelo escolhido está viesado ou não em relação aos dados de validação.